base_config: "TinyStory"
device: "cuda"              # Device: "cuda", "cpu"

# Model Configuration
model:
  vocab_size: 10000           # vocab size for tokenizer
  context_length: 256         # Maximum sequence length
  num_layers: 4               # Number of transformer blocks
  d_model: 512                # Model dimension / embedding size
  num_heads: 16               # Number of attention heads
  d_ff: 1344                  # Feed-forward dimension (typically 4 * d_model)
  rope_theta: 10000.0         # RoPE theta parameter
  eps: 1.0e-5                   # Layer norm epsilon
  dtype: "float32"            # Model dtype: "float32", "float16", "bfloat16"
  use_flash_attention: True  # use customize implemented flash attention or not

# Tokenizer Configuration
tokenizer:
  tokenizer_path: "tokenizer_10000.pkl"  # Path to save/load tokenizer

# Data Configuration
data:
  type: "numpy_data"
  train_data_path: "data/TinyStoriesV2-GPT4-train.npy"    # Memory-mapped training data
  # val_data_path: "data/TinyStoriesV2-GPT4-valid.npy"        # Memory-mapped validation data
  batch_size: 32                              # Training batch size
  # eval_batch_size: 64                         # Evaluation batch size
  # num_workers: 4                              # DataLoader workers

# Training Configuration
training:
  max_steps: 100000           # Maximum training steps
  eval_interval: 1000         # Steps between evaluations
  log_interval: 100           # Steps between logging
  grad_clip_norm: 1.0         # Gradient clipping max norm
  

# Optimizer Configuration
optimizer:
  type: "Muon"               
  lr_muon: 0.02                  # Learning rate of muon
  beta_muon: 0.95                 # beta of muon
  lr_adam: 3.0e-4                 # Learning rate of adam for 1-d parameters
  betas: [0.9, 0.95]            # Adam betas
  eps: 1.0e-10                   # Adam epsilon
  ns_steps: 5
  weight_decay: 0.0

# Learning Rate Scheduler Configuration
scheduler:
  type: "MuonScheduler"    # Scheduler type
  cooldown_frac: 0.4

checkpointing:
  checkpoint_dir: "outputs/checkpoints/muon"        # Directory to save checkpoints
  checkpoint_per_step: 10000                    # when steps % checkpoint_per_step == 0 perform checkpoint
  resume_from: null                   # Path to checkpoint to resume from

logger:
  log_dir: "outputs/tensorboard"